# 《Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks》阅读笔记

## 简介

Fast R-CNN 等模型优化了监测网络，但在 Selective Search 选定 regional proposal 的时候效率太低，于是 Faster R-CNN 提出 RPN 直接卷积特征图预设锚框，把预设部分合并原来的 Fast R-CNN 的检测部分放在一起训练，极大提升了效率与精度。

> 端到端训练：整个系统从输入到输出所有步骤都是可训练且联合优化的。

## 背景

目前获取 regional proposal 的最流行的方法是 selective search，它的核心是贪心算法。

> 先用图像分割算法把图片分割为一些小超像素（即一些颜色和纹理等底层特征相似的部分）。对于所有相邻的超像素对，找出相似度最高的两个区域（_贪心_）合并为新区域，移除这两个区域的所有相关项，把新区域和相邻区域纳入运算，一直循环。

这种运算明显是在 CPU 上跑的，算法本身和不能多线程运算都决定了它慢（CPU 上 2s/图），拖累了整个模型的速度；而且贪心容易陷入局部最优，也处理不了复杂的图像。
新方法 edgeboxes 稍微好一点，但是因为在 CPU 上跑，并且不能结合语义，所以精度效率都不好。
如果实现为在 GPU 上面跑呢？但是不把它和后面的 detection network 架设到一起共享权重，好像浪费了；并且发现 CNN 提取的特征还可以帮助生成 regional proposal ，所以 RPN 就这样干了。

目标检测时多尺度处理是一个重要的部分。常见的三种处理关系：

1. 图像金字塔和特征金字塔
   缩放出来多个尺度的图像，分别提取特征，单独检测（非常慢）
2. 滤波器金字塔
   使用多个不同尺寸或者比例的 filter 来检测不同尺度物体（因为需要事先设定多个滤波器，参数冗余，灵活性差）
3. <span style="color:#ff0000;">参考框（锚框）金字塔</span>
   事先对单尺度图上提取到的特征设定多个 archor（archor 的超参数需要事先定义），对每个 archor 预测分类得分和回归偏移量。灵活且高效率。

Faster R-CNN 采用第三种。

## 实现

Faster R-CNN 由两个部分组成。一个是 Fast R-CNN 的预测模块，一个是 RPN。RPN 就好像“注意力机制”一样提醒预测模块要注意什么，预测出来的结果反向传播时又能够同时训练预测模块和 RPN 的性能。

### RPN

用于特征提取的卷积层是被共享的，然后再分支。
生成候选区域时，用一个 $n \times n$（通常取$ n=3 $）卷积层去对特征图卷积，然后再用两个$ 1 \times 1 $ 卷积层分离特征，流入回归分支和分类分支。

![Fast R-CNN](https://github.com/mingqian-233/CV-Notes/blob/master/notes/images/faster_rcnn.png?raw=true)

#### 锚框

$ 3 \times 3$ 层滑动的时候，会以滑动窗口为中心，生成若干个锚框（这些锚框可以被映射回原图，它们大致捕获了图像的特征）。锚框的个数如下：
\[\text{单点锚框个数} = \text{尺度个数} \times \text{缩放比个数} \\
\text{总锚框个数} = \text{单点锚框个数} \times \text{特征图大小}\]

> 1. 该方法下的锚框具有平移不变性；
> 2. 这种“锚框金字塔”稳定获取多尺度的同时可以节省效率，也有利于共享特征。

#### 损失函数

训练 RPN 时，每个锚点会被分配 Positive/Negative label，遵循规则如下：

1. 与其对应的真实框和它的 IoU 中，是真实框所有匹配到的锚框的 IoU 中最大的；**或者**和对应的真实框的 IoU $\geq$ 0.7，则为正标签；
2. IoU < 0.3，则为负标签；
3. 不符合上述两点的，直接忽视掉。

> 显然这个分配方法在计算 IoU 的时候对应的复杂度为平方级别的$O(NM)$，其中$N$为真实框个数，$M$为锚框个数。但其实一般来说 N 都很小，运算次数并不会很多。

损失函数的计算公式为：

$$
L(\{p_i\}, \{t_i\}) = \frac{1}{N_{\text{cls}}} \sum_i L_{\text{cls}}(p_i, p_i^*) + \lambda \frac{1}{N_{\text{reg}}} \sum_i p_i^* L_{\text{reg}}(t_i, t_i^*)
$$

很好理解，就是把分类的损失和位置回归的损失带权加在一起。
\[
p_i：预测模型的结果，表示
\]
