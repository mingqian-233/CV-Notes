# 计算机视觉基础-极度省流版

## 来源：[d2l](https://d2l.ai/)

### 6. 卷积神经网络（CNN，LeNet）

- 为了处理图片这种二维的这种有位置关系的数据，我们引入卷积层。卷积层对比起全连接层，filter（滤波器/卷积核）组成的卷积层可以处理二维的结构。滤波器在二维矩阵上以步幅（**strides**）滑动，每次滑动进行一次矩阵点乘得到一个数，输出一个新的矩阵。为了保持边缘像素不被丢失，我们在输入的边界周围填充（**padding**）上 0。控制 strides 可以减少输出的高和宽，控制 padding 大小可以使得输入和输出具有相同的高和宽。步幅和填充也可以接受两个参数，一个是高度方向，一个是宽度方向。但一般会相等。
- 以上是二维的情况，可以扩展到第三维：通道。输入通道是把输入分开为几个通道（如 RGB），然后在不同的通道用 filter 的不同的通道处理。filter 的个数可以自己决定（有多少个输出通道就有多少个），但每个 filter 的通道数必须与输入通道数相同。每个 filter 各自将自己的所有通道求和得到输出。
- 卷积层之后一般接 pooling（池化/汇聚）层，目的是让模型更关注“特征”本身而不是“特征在哪里，是不是高糊”。它的运算和 filter 的滑动一样，但是它的运算不是点乘，而是取最大值或者平均值。pooling 层也有填充和步幅。在多通道上，pooling 层对它们单独运算，互不相关。
- LeNet: 最早最简单的 CNN 模型。结构如下。
  ![LeNet](https://d2l.ai/_images/lenet.svg)

### 7. 现代卷积神经网络

#### AlexNet

加深了网络的层数，由五个卷积层、两个全连接隐藏层和一个全连接输出层组成并且用 ReLU 替换了 sigmoid 作为激活函数。最后全连接层参数巨大（接近 1GB），当年的硬件条件需要跨 GPU 分解模型，但现在不需要。控制模型复杂度时，对比起 LeNet 用的权重衰减，AlexNex 用了 Dropout。喂数据时 AlexNet 用了翻转、裁切、变色等扩增数据。

#### VGG

思想是把一系列卷积层打包成块，开创了“积木式”的模型设计，方便重复调用增加深度。用多个小 filter 堆叠代替大 filter，减少了参数量且增强了非线性能力。

#### NiN

创新点在于在每个卷积层后使用两个 1\*1 卷积层形成 mlpconv 层（在不破坏空间结构的前提下类似全连接层的效果，跨通道整合信息）以及用 GAP(global average pooling)取代最后的全连接层（GAP 是将每个通道的特征图取平均得到一个长度为通道数的向量）

#### GoogLeNet

使用 Inception 块作为基本的卷积块，由四条并行路径组成；模型的 Inception 块之间穿插 Pooling 层来降维。最后仍然是使用 GAP 替代 FC 层。

#### ResNet

把训练目标改为拟合出残差映射$f(x)-x$来使其更容易优化。同时可以使得正向传播的特征保留、反向传播的梯度无衰减传递。  
![ResNet](https://d2l.ai/_images/residual-block.svg)

#### DenseNet

ResNet 的本质是把目标函数$f$分解为一个简单的线性项和一个复杂的非线性项，即$f(x)=x+g(x)$。DenseNet 试图把$f$拓展为超过两部分的信息，用一个“接力”的方式不断把输出以一种“连接”的形式，传递给后来的层。这样设计会大大增强模型鲁棒性。

```
（假设有5层）
层4输出 → 层5输入
层3输出 → 层4输入 + 层5输入
层2输出 → 层3输入 + 层4输入 + 层5输入
层1输出 → 层2输出 + 层3输入 + 层4输入 + 层5输入
```

这样处理，通道数会一直增长，所以 DenseNet 需要用过渡层（1×1 卷积降维、池化）缩小尺寸。其中池化运用平均，因为每个 Dense Block 中累计了以前的所有信息，平均池化可以保证过渡平稳。

### 13.计算机视觉

#### 图像增广

榨干数据集的剩余价值。对训练集，使用翻转和裁剪、改变颜色或者相结合的方式，提升其泛化能力。预测集不使用。

#### 微调

拿来别人的训练好的模型（参数），把输出层以外的学习率调低，根据需求改变输出数并把输出层的学习率调高。

#### 目标检测和边界框

略

#### 锚框（anchor box）

目标检测先采样大量区域，再判断区域内是否包含目标，从而能更准确预测目标。以每个像素为中心，生成多个缩放比和宽高比（aspect ratio）不同的边界框，这些边界框被称为锚框（anchor box）。交并比表示重合程度，即相交面积/相并面积。锚框会找到和自己交并比最高的配对。每个真实框找和自己匹配（阈值一般为 0.5）的锚框匹配，一个锚框只能有一个真实框。预测时，用非极大值抑制（NMS）合并属于同一目标的类似的预测边界框。

> NMS：取出冗余的预测框，确保同一目标只由最精确的一个检测框表示。否则会被重复检测，生成杂乱的结果。

#### 多尺度目标检测

生成太多锚框是不现实的，所以在多尺度下（为了检测不同大中小的目标）生成多个均匀分布的、宽高比不同的锚框，再利用这些锚框进行多尺度检测。

#### 目标检测数据集

略

#### 单发多框检测（SSD）

单发指单次前向传播，多框指多个提前生成的锚框。分为类别预测层和边界框预测层。之后为了提高效率，连结多尺度的预测（通过相同的 batch_size 作为第一维连接，第二维为高 × 宽 × 通道数），通过高和宽减半块将输入特征图的高和宽减半（这会改变通道的数量），扩大每个单元在其输出特征图中的感受野。类别损失函数采用交叉熵损失，偏移量损失函数用$L_1$范数损失（差的绝对值）

#### R-CNN（region-based CNN/regions with CNN features）系列

- **基础的 R-CNN**
  选择性搜索通过传统图像处理方法选取多个高质量的提议区域（region proposal），通常在多个尺度下选取，形状大小不同。将它们缩放后进入 CNN 提取特征（不要输出层），训练支持向量机（SVM）对提取到的特征进行分类，其中每一个 SVM 用于判断特征是否属于某类别。最后边界框回归预测真实边界框。这种方法效率低下，因为选择性搜索可能选出特别多的提议区域。

- **Fast R-CNN**
  对每个 region proposal，基础的 R-CNN 处理它们时，CNN 的前向传播独立，不共享计算。但是这些区域经常重叠，重复计算造成了浪费。Fast R-CNN 的改进是在整张图上进行 CNN 的前向传播，提取特征，然后和 region proposals 一起被塞进**ROI Pooling**层（用于将任意大小的候选区域缩放成固定尺寸，保留主要特征）。最后是全连接层，同样是分别处理位置偏移和分类（概率）误差。

  > ROI Pooling：指定每个区域输出的高和宽分别为$h_2$和$w_2$。对于任何形状为 $h \times w$ 的兴趣区域窗口，该窗口将被**划分**（**_即不再是卷积一样的滑动，而是单纯的分割_**）为 $h_2 \times w_2$ 子窗口网格，其中每个子窗口的大小约为 $\frac{h}{h_2} \times \frac{w}{w_2}$（优先向上取整）。

- **Faster R-CNN**
  Fast R-CNN 有一个劣势在于需要生成大量的 region proposal 减慢了效率。对比起 Fast R-CNN，Faster R-CNN 的唯一不同是把选择性搜索替换为区域提议网络（**RPN**），减少 region proposal 的生成数量。  
   ![Faster R-CNN](https://d2l.ai/_images/faster-rcnn.svg)

  > RPN：用 3\*3 卷积层(_padding=1_)处理 CNN 的输出，输出的通道数记为 c，这样特征图中每个单元均得到一个长度为 c 的新特征。以特征图的每个像素为中心，自动生成并标注多个锚框。然后按照之前介绍的步骤进入类别预测和位置偏移预测（使用非极大值抑制移除相似的）

- **Mask R-CNN**
  训练集如果标注了每个目标在图像上的像素级位置（像 PS 抠图抠出一整块像素然后知道这一片都是狗的那种）就可以利用这些信息提升目标检测精度。在 Faster R-CNN 的基础上，Mask R-CNN 把 ROI Pooling 改为 ROI align，利用双线性插值（bilinear interpolation）保留空间特征信息，更适合像素级预测。另外引入了全卷积层（FC）达成像素级的目标预测，后续会详细介绍 FC。网络设计如下：  
   ![Mask R-CNN](https://d2l.ai/_images/mask-rcnn.svg)
  > 双线性插值：通过周围四个已知点，在水平和垂直方向上分别进行线性插值，估算某一点。对比起 ROI Pooling，它不进行向上取整，更能精确到像素。

#### 转置卷积（Transposed Convolution）

前面的这些 CNN 相关的 layer 全部都是把数据降维或者不变的。但是语义分割任务需要将每个像素分配到一个类别，保持输出和输出的空间维度一致会更方便，因此引入一个可以升维的层——转置卷积（Transposed Convolution）层。
ConvTranspose 层类似于可以放大细节的放大镜。细节如下：

- 输入通过 kernel 的时候，每一个元素广播后和 kernel 相乘，形成一个小矩阵，每一个小矩阵通过下面的 padding 和 stride 的控制相加形成结果的大矩阵
- 参数有 padding 和 stride，都用于输出（常规的是用于输入的）
- padding 指裁剪掉输出周围的多少层，stride 指在卷积过程中，输入和 kernel 相乘之后得到的小矩阵的滑动步幅。步幅作用如图：
  ![stride](https://d2l.ai/_images/trans_conv_stride2.svg)
- “转置”名称的由来，是因为普通卷积可以看待为矩阵乘法$\mathbf{y}=\mathbf{W}\mathbf{x}$，反向传播函数$\nabla_{\mathbf{x}}\mathbf{y}=\mathbf{W}^\top$；而转置卷积层的作用类似于把上述操作调转，正向传播与$\mathbf{W}^\top$相乘，因此得名。

#### 全卷积网络（FCN）

因为 FCN 的输出结果是对每一个像素的预测，所以需要将中间层特征图的高和宽变成输入图像的尺寸，由上述的 ConvTranspose 实现。
首先用常规的卷积神经网络提取图像特征（不要池化层、输出层），然后用 1\*1 的卷积层转换通道数（为分类数）。最后需要通过转置卷积层把特征图放大。
训练前用提取到的特征初始化 ConvTranspose 层（_假如随机初始化会浪费大量训练资源_），需要把下采样得到的特征图放大，即 upsampling，可以使用上面提到过的双线性插值。

#### 风格迁移

略
